---
title: "PCS Documentation"
author: ""
date: "`r format(Sys.time(), '%B %d, %Y')`"
# bibliography: bibliography.bib
header-includes:
    - \usepackage{float}
    - \usepackage{amsmath}
    - \usepackage{gensymb}
output:
  # rmdformats::html_clean:
  # rmdformats::readthedown:
  rmdformats::material:
    fig_caption: true
    code_folding: hide
    number_sections: true
    use_bookdown: true
    fig_width: 10
    fig_height: 8
    lightbox: true
css: css/custom_material_rmd_theme.css
# runtime: shiny
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  # cache = TRUE,
  collapse = TRUE,
  fig.align = "center",
  fig.pos = "H",
  fig.show = "hold",
  comment = "#>"
)

# load in packages
library(tidyverse)
library(skimr)
library(R.utils)
library(datasets)
library(ggplot2)
library(dplyr)
library(tidyr)
library(reshape2)
for (f in list.files("R", pattern = ".R", full.names = TRUE)) {
  source(f, chdir = T)
}

chunk_idx <- 1
```

# Domain problem formulation

<input type="checkbox" unchecked> What is the real-world question? This could be hypothesis-driven or discovery-based.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Why is this question interesting and important? What are the implications of better understanding this data?</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Briefly describe any background information necessary to understand this problem.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Briefly describe how this question can be answered in the context of a model or analysis.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Outline the rest of the report/analysis.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::


# Data

<input type="checkbox" unchecked> What is the data under investigation? Provide a brief overview/description of the data.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Describe how your data connects to the domain problem.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

## Data Collection

<input type="checkbox" unchecked> How was the data collected or generated (including details on the experimental design)? Be as transparent as possible so that conclusions made from this data are not misinterpreted down the road.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Describe any limitations when using the data to answer the domain problem of interest.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Where is the data stored, and how can it be accessed by others (if applicable)?</input>


::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

## Data splitting

TODO: add advice for possible data splits

<input type="checkbox" unchecked> Split the data into a training, validation, and test set.</input>

<input type="checkbox" unchecked> Decide on the proportion of data in each split.</input>

<input type="checkbox" unchecked> Decide on the "how" to split the data (e.g., random sampling, stratified sampling, etc.), and explain why this is a reasonable way to split the data.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Provide summary statistics and/or figures of the three data sets to illustrate how similar (or different) they are.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

### Data Split Overview
```{r load-data}
# TODO: pick more interesting datasets
data = iris
breakdown = c(train = 0.6, validate = 0.2, test = 0.2)
labels = sample(cut(
  seq(nrow(data)), 
  nrow(data)*cumsum(c(0,breakdown)),
  labels = names(breakdown)
))
data_split = split(data, labels)

Xtrain <- data_split$train %>% dplyr::select(-Species)
Xvalid <- data_split$validate %>% dplyr::select(-Species)
Xtest <- data_split$test %>% dplyr::select(-Species)
ytrain <- data_split$train$Species
yvalid <- data_split$validate$Species
ytest <- data_split$test$Species

######FIX need "splits" for tuning code
X <- iris %>% dplyr::select(-Species)
y <- iris$Species
data_df <- dplyr::bind_cols(.y = y, X)
splits <- rsample::initial_split(data_df)
train_df <- rsample::training(splits)
valid_df <- rsample::testing(splits)
```

###  {.unnumbered .tabset .tabset-pills .tabset-fade}

#### X Comparison {.unnumbered}
```{r compare-X}
train = melt(data_split$train)
train$split = "train"
validation = melt(data_split$validate)
validation$split = "validation"
test = melt(data_split$test)
test$split = "test"
data_all = rbind(train,validation,test)
data_all$split = factor(data_all$split,levels=c("train","validation","test"))
fnt = 12
fnt2=12
#ggplot(data_all, aes(x=variable, y=value)) + 
 # geom_boxplot() + 
  ggplot(data_all, aes(x=value, fill=variable)) + geom_density(alpha=.3)+
    theme_bw() + 
  facet_wrap(. ~ split)+
  theme(axis.title.x = element_text(size=fnt,face="bold")) + 
  #theme(axis.title.y = element_text(size=fnt,face="bold")) +
  theme(axis.title.y = element_blank())+
  theme(strip.text.x = element_text(size = fnt2,face="bold")) +
  #ylab("True Positives") +
  xlab("") + 
  theme(legend.title = element_blank())+#element_text(size=fnt,face="bold")) +
  theme(legend.text=element_text(size=fnt2,face="bold"))
  
```

#### Y Comparison {.unnumbered}
```{r compare-Y}
plot_data <- data_all %>% dplyr::select(c(Species,split)) %>%
      count(Species,split) %>% 
      group_by(split) %>% 
      mutate(percent = n/sum(n))
ggplot(plot_data, aes(x = split, y = percent, fill = Species)) + 
  theme_bw() + 
      geom_col(position = "fill") +
  theme(axis.title.x = element_text(size=fnt,face="bold")) + 
  #theme(axis.title.y = element_text(size=fnt,face="bold")) +
  theme(axis.title.y = element_blank())+
  theme(strip.text.x = element_text(size = fnt2,face="bold")) +
  #ylab("True Positives") +
  xlab("") + 
  theme(legend.title = element_blank())+#element_text(size=fnt,face="bold")) +
  theme(legend.text=element_text(size=fnt2,face="bold"))
```
## Data Cleaning and Preprocessing

<input type="checkbox" unchecked> What steps were taken to clean the data? More importantly, why was the data cleaned in this way?</input>

<input type="checkbox" unchecked> Discuss all inconsistencies, problems, oddities in the data (e.g., missing data, errors in data, outliers, etc.).</input>

<input type="checkbox" unchecked> Record your preprocessing steps in a way such that if someone else were to reproduce your analysis, they could easily replicate and understand your steps.</input>

<input type="checkbox" unchecked> It can be helpful to include relevant plots that explain/justify the choices that were made when cleaning the data.</input>

<input type="checkbox" unchecked> If more than one preprocessing pipeline is reasonable, examine the impacts of these alternative preprocessing pipelines on the final data results.</input>

<input type="checkbox" unchecked> Again, be as transparent as possible. This allows others to make their own educated decisions on how best to preprocess the data.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

## Data Exploration

TODO: Add drag and drop feature in shiny version for other images

The main goal of this section is to give the reader a feel for what the data "looks like" at a basic level.

<input type="checkbox" unchecked> Provide plots that summarize the data and perhaps even plots that convey some smaller findings which ultimately motivate the main findings.</input>

<input type="checkbox" unchecked> Provide additional plots representing remaining oddities after pre-processing if applicable.</input>

<input type="checkbox" unchecked> Add summary statistics in accompanying tables (or in figures) for quick comparisons.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

### Data Overview

```{r echo = FALSE}
if (length(ytrain) != nrow(Xtrain)) {
  text_out <- paste0(
    "Warning: Number of samples in Xtrain (n = ", nrow(Xtrain), ") ",
    "not equal to number of samples in ytrain (n = ", length(ytrain), ")."
  )
} else {
  text_out <- paste0(
    paste0("Number of samples: ", nrow(Xtrain), "\n"),
    paste0("Number of features: ", ncol(Xtrain), "\n"),
    paste0("Number of NAs in training y: ", sum(is.na(ytrain)), "\n"),
    paste0("Number of NAs in training X: ", sum(is.na(Xtrain)), "\n"),
    paste0("Number of columns in training X with NAs: ", 
           sum(apply(Xtrain, 2, FUN = function(x) any(is.na(x)))), 
           "\n"),
    paste0("Number of constant columns in training X: ",
           sum(apply(Xtrain, 2,
                     FUN = function(x) {
                       all(x == x[!is.na(x)][1], na.rm = T)
                     }), na.rm = T)))
}
cat(text_out)
```

###  {.unnumbered .tabset .tabset-pills .tabset-fade}

#### Summary Tables {.unnumbered}

```{r results="asis"}
data_types(Xtrain = Xtrain, ytrain = ytrain)
```

```{r results="asis"}
dt_ls <- data_summary(Xtrain = Xtrain, ytrain = ytrain, digits = 2, sigfig = F)
for (dt_name in names(dt_ls)) {
  subchunkify(dt_ls[[dt_name]], i = chunk_idx, other_args = "results='asis'")
  chunk_idx <- chunk_idx + 1
}
```

#### X distribution {.unnumbered}

```{r}
# plot X distribution
plot_X_distribution(Xtrain, "density")
```

#### Y distribution {.unnumbered}

```{r}
# plot y distribution
plot_y_distribution(ytrain, "bar")
```

#### Feature Correlation {.unnumbered}

```{r}
# correlation heatmap
plotCorHeatmap(X = Xtrain, cor.type = "pearson", clust = TRUE, text.size = 8)
```

#### Feature Pair Plots {.unnumbered}

```{r}
# pair plots
col_ids <- 1:min(ncol(Xtrain), 6)
plotPairs(data = Xtrain, columns = col_ids, 
          color = ytrain, color.label = "y")
```

#### Marginal Association Plots {.unnumbered}

```{r}
caret::featurePlot(x = Xtrain,
                   y = ytrain,
                   plot = if (is.factor(ytrain)) "box" else "scatter",
                   # strip = strip.custom(par.strip.text = list(cex = .7)),
                   scales = list(x = list(relation = "free"), 
                                 y = list(relation = "free")))
```

#### PCA Plots {.unnumbered}

```{r}
# dimension reduction plots
plotPCA(X = Xtrain, npcs = 3, color = ytrain, color.label = "y",
        center = T, scale = FALSE)$plot
```

For inspiration: [Shiny App](https://tiffanymtang.shinyapps.io/ShinyEDA/)

# Prediction Modeling

TODO: add advice on which models to select and why

<input type="checkbox" unchecked> Discuss the prediction methods under consideration, and explain why these methods were chosen.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Discuss the accuracy metrics under consideration, and explain why these metrics were chosen.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

Note: there should be multiple methods and metrics under consideration to paint a more holistic picture of the data. At least one method should be a baseline, common approach that may not be optimal for the problem setting, but serves as a helpful comparison.

## Prediction check

<input type="checkbox" unchecked> Carry out the prediction pipeline, outlined above.</input>

    -   Fit prediction methods on training data.
    -   Evaluate prediction methods on validation data.
    -   Compare results, and filter out poor models.

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

###  {.unnumbered .tabset .tabset-pills .tabset-fade}

#### tidymodels {.unnumbered}

```{r tidymodels-fits}
# TODO: add code for tuning parameters
mod_recipe <- recipes::recipe(.y ~., data = splits)

# for classification
rf_model <- parsnip::rand_forest() %>%
  parsnip::set_args(mtry = tune::tune()) %>%
  parsnip::set_engine("ranger", importance = "impurity") %>%
  parsnip::set_mode("classification")
rf_grid <- tidyr::crossing(mtry = 1:4)

svm_model <- parsnip::svm_rbf() %>%
  parsnip::set_engine("kernlab") %>%
  parsnip::set_mode("classification")

knn_model <- parsnip::nearest_neighbor() %>%
   parsnip::set_args(neighbors = tune(), weight_func = tune()) %>% 
   parsnip::set_engine("kknn") %>% 
   parsnip::set_mode("classification")

# models <- workflowsets::workflow_set(
#   preproc = list(Base = mod_recipe),
#   models = list(RF = rf_model, SVM = svm_model, KNN = knn_model),
#   cross = TRUE
# ) %>%
#   workflowsets::option_add(grid = rf_grid, id = "Base_RF")
# model_fits <- workflowsets::workflow_map(
#   object = models,
#   fn = "tune_grid"
# )

model_list <- list(RF = list(model = rf_model,
                             grid = rf_grid), 
                   SVM = list(model = svm_model,
                              grid = NULL), 
                   KNN = list(model = knn_model,
                              grid = 4))
model_fits <- list()
model_preds <- list()
model_errs <- list()
model_vimps <- list()
for (model_name in names(model_list)) {
  mod <- model_list[[model_name]]$model
  grid <- model_list[[model_name]]$grid
  if (!is.null(grid)) {
    mod_fit <- workflows::workflow() %>%
      workflows::add_recipe(mod_recipe) %>%
      workflows::add_model(mod)
    best_params <- mod_fit %>%
      tune::tune_grid(resamples = rsample::vfold_cv(train_df), 
                      grid = grid) %>%
      tune::select_best(metric = "accuracy")
    mod_fit <- mod_fit %>%
      tune::finalize_workflow(best_params) %>%
      tune::last_fit(splits)
  } else {
    mod_fit <- workflows::workflow() %>%
      workflows::add_recipe(mod_recipe) %>%
      workflows::add_model(mod) %>%
      tune::last_fit(splits)
  }
  model_fits[[model_name]] <- mod_fit
  model_preds[[model_name]] <- mod_fit %>%
    tune::collect_predictions()
  model_errs[[model_name]] <- mod_fit %>%
    tune::collect_metrics()
  model_vimps[[model_name]] <- tryCatch({
    # model-specific variable importance
    mod_fit %>%
      workflows::extract_fit_parsnip() %>%
      vip::vi()
  }, error = function(e) {
    # model-agnostic permutation variable importance
    mod_fit %>%
      workflows::extract_fit_parsnip() %>%
      vip::vi(method = "permute", train = train_df, target = ".y",
              feature_names = setdiff(colnames(train_df), ".y"), 
              pred_wrapper = predict, metric = "accuracy")
  })
}
model_preds <- dplyr::bind_rows(model_preds, .id = "model")
model_errs <- dplyr::bind_rows(model_errs, .id = "model")
model_vimps <- dplyr::bind_rows(model_vimps, .id = "model")

```

```{r results = "asis", echo = FALSE}
# table of accuracy metrics
model_errs %>%
  dplyr::select(-.estimator, -.config) %>%
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %>%
  prettyDT(digits = 2, sigfig = F, rownames = FALSE, 
           caption = "Validation Prediction Accuracies", 
           bold_function = ". == max(., na.rm = TRUE)", bold_margin = 2,
           bold_scheme = c(F, rep(T, ncol(.) - 1)),
           options = list(dom = 't'))

```

#### caret {.unnumbered}

```{r caret-fits, eval = FALSE }
# how to do cross validation
trcontrol <- caret::trainControl(
  method = "cv",
  number = 5,
  classProbs = if (is.factor(ytrain)) TRUE else FALSE,
  summaryFunction = caret::defaultSummary,
  allowParallel = FALSE,
  verboseIter = FALSE
)

response <- "raw"
model_list <- list(
  ranger = list(tuneGrid = expand.grid(mtry = seq(sqrt(ncol(Xtrain)), 
                                                  ncol(Xtrain) / 3,
                                                  length.out = 3),
                                       splitrule = "gini",
                                       min.node.size = 1),
                importance = "impurity",
                num.threads = 1),
  xgbTree = list(tuneGrid = expand.grid(nrounds = c(10, 25, 50, 100, 150),
                                        max_depth = c(3, 6),
                                        colsample_bytree = 0.33,
                                        eta = c(0.1, 0.3),
                                        gamma = 0,
                                        min_child_weight = 1,
                                        subsample = 0.6),
                 nthread = 1)
)

model_fits <- list()
model_preds <- list()
model_errs <- list()
model_vimps <- list()
for (model_name in names(model_list)) {
  mod <- model_list[[model_name]]
  if (identical(mod, list())) {
    mod <- NULL
  }
  mod_fit <- do.call(caret::train, args = c(list(x = as.data.frame(Xtrain),
                                                 y = ytrain,
                                                 trControl = trcontrol,
                                                 method = model_name),
                                            mod))
  model_fits[[model_name]] <- mod_fit
  model_preds[[model_name]] <- predict(mod_fit, as.data.frame(Xvalid),
                                       type = response)
  model_errs[[model_name]] <- caret::postResample(
    pred = model_preds[[model_name]], obs = yvalid
  )
  model_vimps[[model_name]] <- caret::varImp(mod_fit)
}
model_preds <- dplyr::bind_rows(model_preds, .id = "model")
model_errs <- dplyr::bind_rows(model_errs, .id = "model")
model_vimps <- purrr::map_dfr(model_vimps, 
                              ~.x[["importance"]] %>% 
                                tibble::rownames_to_column("variable"),
                              .id = "model")

```

```{r results = "asis", echo = FALSE}
# table of accuracy metrics
model_errs %>%
  prettyDT(digits = 2, sigfig = F, rownames = FALSE, 
           caption = "Validation Prediction Accuracies", 
           bold_function = ". == max(., na.rm = TRUE)", bold_margin = 2,
           bold_scheme = c(F, rep(T, ncol(.) - 1)),
           options = list(dom = 't'))

```

#### h20 {.unnumbered}

```{r h2o-fits, eval = FALSE}
library(h2o)
h2o.init(nthreads = 1)
iris.hex <- as.h2o(iris)
splits <- h2o.splitFrame(data = iris.hex,
                         ratios = c(0.8))
train_df <- splits[[1]]
valid_df <- splits[[2]]

model_list <- list(randomForest = list(ntrees = 500), 
                   xgboost = list())

model_fits <- list()
model_preds <- list()
model_errs <- list()
model_vimps <- list()
for (model_name in names(model_list)) {
  mod <- model_list[[model_name]]
  if (identical(mod, list())) {
    mod <- NULL
  }
  mod_fit <- do.call(paste0("h2o.", model_name),
                     args = c(list(x = colnames(Xtrain),
                                   y = "Species",
                                   training_frame = train_df,
                                   model_id = model_name),
                              mod))
  model_fits[[model_name]] <- mod_fit
  model_preds[[model_name]] <- h2o.predict(mod_fit, valid_df)
  model_errs[[model_name]] <- h2o.performance(mod_fit, valid_df)
  model_vimps[[model_name]] <- h2o.varimp(mod_fit)
}
model_preds <- purrr::map_dfr(model_preds, ~attr(.x, "data"), .id = "model")
model_errs <- purrr::map_dfr(
  model_errs, 
  function(err) {
    rm_objs <- c("model", "model_checksum", "frame", "frame_checksum",
                 "description", "scoring_time", "predictions")
    simChef:::simplify_tibble(simChef:::list_to_tibble_row(
      err@metrics[setdiff(names(err@metrics), rm_objs)]
    ))
  }, 
  .id = "model"
)
model_vimps <- dplyr::bind_rows(model_vimps, .id = "model")

```

```{r results = "asis", echo = FALSE}
# table of accuracy metrics
model_errs %>%
  prettyDT(digits = 2, sigfig = F, rownames = FALSE, 
           caption = "Validation Prediction Accuracies", 
           options = list(dom = 't'))

```

## Stability check

<input type="checkbox" unchecked> Taking the prediction methods that pass the prediction check, perform stability analysis.</input>

    -   Specify and justify the appropriate data perturbation(s).
    -   Re-fit the prediction methods on these perturbed data sets.
    -   Evaluate prediction methods on validation data.
    -   Assess stability across the data perturbations as well as across the various methods.
    -   Filter out poor models where necessary and interpret stability results.

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

TODO: Add results for tidymodels and h20 in addition to caret
<!-- TODO: Ana - Provide some example code here (for both fitting and visualizing results) so that the practitioner can easily input their data and models. Add something like James' slide. A few pictures with possible data perturbation schemes (separate from parameter tuning). Data splitting vs sampling of observations. Cross-validation-ish scheme and fixed training/validation setup (bootstrapping, sub-sampling, stratified-sampling). Add parameter to include/exclude certain code chunks. -->

###  {.unnumbered .tabset .tabset-pills .tabset-fade}

#### tidymodels {.unnumbered}

#### caret {.unnumbered}
```{r caret-fits-stability, eval = FALSE }
# bootstrap training
bootstrap = sample(1:nrow(Xtrain),nrow(Xtrain))
print(bootstrap)
Xtrain_b = Xtrain[bootstrap,]
ytrain_b = ytrain[bootstrap]

# how to do cross validation
trcontrol <- caret::trainControl(
  method = "cv",
  number = 5,
  classProbs = if (is.factor(ytrain_b)) TRUE else FALSE,
  summaryFunction = caret::defaultSummary,
  allowParallel = FALSE,
  verboseIter = FALSE
)

response <- "raw"
model_list <- list(
  ranger = list(tuneGrid = expand.grid(mtry = seq(sqrt(ncol(Xtrain_b)), 
                                                  ncol(Xtrain) / 3,
                                                  length.out = 3),
                                       splitrule = "gini",
                                       min.node.size = 1),
                importance = "impurity",
                num.threads = 1),
  xgbTree = list(tuneGrid = expand.grid(nrounds = c(10, 25, 50, 100, 150),
                                        max_depth = c(3, 6),
                                        colsample_bytree = 0.33,
                                        eta = c(0.1, 0.3),
                                        gamma = 0,
                                        min_child_weight = 1,
                                        subsample = 0.6),
                 nthread = 1)
)

model_fits <- list()
model_preds <- list()
model_errs <- list()
model_vimps <- list()
for (model_name in names(model_list)) {
  mod <- model_list[[model_name]]
  if (identical(mod, list())) {
    mod <- NULL
  }
  mod_fit <- do.call(caret::train, args = c(list(x = as.data.frame(Xtrain_b),
                                                 y = ytrain_b,
                                                 trControl = trcontrol,
                                                 method = model_name),
                                            mod))
  model_fits[[model_name]] <- mod_fit
  model_preds[[model_name]] <- predict(mod_fit, as.data.frame(Xvalid),
                                       type = response)
  model_errs[[model_name]] <- caret::postResample(
    pred = model_preds[[model_name]], obs = yvalid
  )
  model_vimps[[model_name]] <- caret::varImp(mod_fit)
}

model_preds <- dplyr::bind_rows(model_preds, .id = "model")
model_errs <- dplyr::bind_rows(model_errs, .id = "model")
model_vimps <- purrr::map_dfr(model_vimps, 
                              ~.x[["importance"]] %>% 
                                tibble::rownames_to_column("variable"),
                              .id = "model")

```

```{r results = "asis", echo = FALSE}
# table of accuracy metrics
model_errs %>%
  prettyDT(digits = 2, sigfig = F, rownames = FALSE, 
           caption = "Validation Prediction Accuracies Over Bootstrapped Training Fits", 
           bold_function = ". == max(., na.rm = TRUE)", bold_margin = 2,
           bold_scheme = c(F, rep(T, ncol(.) - 1)),
           options = list(dom = 't'))

```
#### h20 {.unnumbered}


## Interpretability

<input type="checkbox" unchecked> For the models that pass the prediction and stability checks,</input>

    -   Extract the important features in the predictive models that are stable across both data and model perturbations. Determining the importance of a feature can be method dependent.

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<!-- TODO: Tiffany - Provide some example code here (for both fitting and visualizing results) so that the practitioner can easily input their data and models. -->

### Without stability

```{r}
prettyDT(model_vimps, digits = 2, sigfig = F, caption = "Variable Importances")
```

```{r}
# bar plot
vip::vip(model_vimps,
         num_features = 10,
         geom = "col") +
  prettyGGplotTheme()
```

```{r}
# scatter plot
plt <- model_vimps %>%
  tidyr::pivot_wider(names_from = "model", values_from = "Importance") %>%
  plotPairs(columns = which(!(colnames(.) %in% "Variable"))) +
  ggplot2::theme_bw() 
plotly::ggplotly(plt)
```

### With stability

TODO

# Main Results

<input type="checkbox" unchecked> Interpret and summarize the prediction and stability results.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<input type="checkbox" unchecked> Evaluate pipeline on test data.</input>

<input type="checkbox" unchecked> Summarize test set prediction and/or interpretability results.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

<!-- TODO: Ana - add template tables with interpretation -->
###  {.unnumbered .tabset .tabset-pills .tabset-fade}

#### tidymodels {.unnumbered}

#### caret {.unnumbered}
```{r caret-finalfits, eval = FALSE }
# how to do cross validation
trcontrol <- caret::trainControl(
  method = "cv",
  number = 5,
  classProbs = if (is.factor(ytrain)) TRUE else FALSE,
  summaryFunction = caret::defaultSummary,
  allowParallel = FALSE,
  verboseIter = FALSE
)

response <- "raw"
model_list <- list(
  ranger = list(tuneGrid = expand.grid(mtry = seq(sqrt(ncol(Xtrain)), 
                                                  ncol(Xtrain) / 3,
                                                  length.out = 3),
                                       splitrule = "gini",
                                       min.node.size = 1),
                importance = "impurity",
                num.threads = 1),
  xgbTree = list(tuneGrid = expand.grid(nrounds = c(10, 25, 50, 100, 150),
                                        max_depth = c(3, 6),
                                        colsample_bytree = 0.33,
                                        eta = c(0.1, 0.3),
                                        gamma = 0,
                                        min_child_weight = 1,
                                        subsample = 0.6),
                 nthread = 1)
)

model_fits <- list()
model_preds <- list()
model_errs <- list()
model_vimps <- list()
for (model_name in names(model_list)) {
  mod <- model_list[[model_name]]
  if (identical(mod, list())) {
    mod <- NULL
  }
  mod_fit <- do.call(caret::train, args = c(list(x = as.data.frame(Xtrain),
                                                 y = ytrain,
                                                 trControl = trcontrol,
                                                 method = model_name),
                                            mod))
  model_fits[[model_name]] <- mod_fit
  model_preds[[model_name]] <- predict(mod_fit, as.data.frame(Xtest),
                                       type = response)
  model_errs[[model_name]] <- caret::postResample(
    pred = model_preds[[model_name]], obs = ytest
  )
  model_vimps[[model_name]] <- caret::varImp(mod_fit)
}
model_preds <- dplyr::bind_rows(model_preds, .id = "model")
model_errs <- dplyr::bind_rows(model_errs, .id = "model")
model_vimps <- purrr::map_dfr(model_vimps, 
                              ~.x[["importance"]] %>% 
                                tibble::rownames_to_column("variable"),
                              .id = "model")

```

```{r results = "asis", echo = FALSE}
# table of accuracy metrics
model_errs %>%
  prettyDT(digits = 2, sigfig = F, rownames = FALSE, 
           caption = "Test Prediction Accuracies", 
           bold_function = ". == max(., na.rm = TRUE)", bold_margin = 2,
           bold_scheme = c(F, rep(T, ncol(.) - 1)),
           options = list(dom = 't'))

```
#### h20 {.unnumbered}

# Post hoc analysis

<input type="checkbox" unchecked> Move beyond the global prediction accuracy metrics and dive deeper into individual-level predictions for the validation and/or test set, i.e., provide a more "local" analysis.</input>

    -   Examine any points that had poor predictions.
    -   Examine differences between prediction methods.

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

TODO: Tiffany - Add examples with interesting observations of prediction accuracy metrics so the user knows what to look for.

```{r}

model_preds %>%
  tidyr::pivot_wider(names_from = "model", values_from = ".pred_setosa", 
                     id_cols = c("id", ".row")) %>%
  plotPairs(columns = which(!(colnames(.) %in% c("id", ".row"))))

```

# Conclusions

<input type="checkbox" unchecked> Reiterate main findings, note any caveats, and clearly translate findings/analysis back to the domain problem context.</input>

::: {contenteditable="true" .panel .panel-default .padded-panel}
Insert narrative here.
:::

# Bibliography {.unnumbered}
