---
title: "PCS Documentation"
author: ""
date: "`r format(Sys.time(), '%B %d, %Y')`"
# bibliography: bibliography.bib
header-includes:
    - \usepackage{float}
    - \usepackage{amsmath}
    - \usepackage{gensymb}
output:
  # rmdformats::html_clean:
  rmdformats::readthedown:
  # rmdformats::material:
    fig_caption: true
    code_folding: hide
    number_sections: true
    use_bookdown: true
    fig_width: 8
    fig_height: 6
    lightbox: true
css: css/custom_htmlclean_rmd_theme.css
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  fig.align = "center",
  fig.pos = "H",
  fig.show = "hold"
)

# load in packages
library(tidyverse)

```

# Domain problem formulation

-   What is the real-world question? This could be hypothesis-driven or discovery-based.
-   Why is this question interesting and important? What are the implications of better understanding this data?
-   Is there any prior work or background information related to this question? Describe the premise and put your questions and analysis in the domain context.
-   Briefly describe how this question can be answered in the context of a model or analysis.
-   Outline the rest of the report/analysis.

# Data

-   What is the data under investigation? Provide a brief overview/description of the data.
-   How is this data relevant to the problem of interest? In other words, make the link between the data and the domain problem.

## Data Collection

-   How was the data collected or generated (including details on the experimental design)? Be as transparent as possible so that conclusions made from this data are not misinterpreted down the road.
-   Describe what the data represents in reality, i.e., make the link between the data and reality.
-   Where is the data stored, and how can it be accessed by others (if applicable)?

## Data Cleaning and Preprocessing

-   What steps were taken to clean the data? More importantly, why was the data cleaned in this way?
-   Discuss all inconsistencies, problems, oddities in the data (e.g., missing data, errors in data, outliers, etc.).
-   Record your preprocessing steps in a way such that if someone else were to reproduce your analysis, they could easily replicate and understand your preprocessing.
-   It can be helpful to include relevant plots that help to explain/justify the choices that were made when cleaning the data.
-   If more than one preprocessing pipeline is reasonable, examine the impacts of these alternative preprocessing pipelines on the final data results.
-   Again, be as transparent as possible. This allows others to make their own educated decisions on how best to preprocess the data.

## Data Exploration

-   The main goal of this section is to give the reader a feel for what the data "looks like" at a basic level.
-   Provide plots that summarize the data and perhaps even plots that convey some smaller findings which ultimately motivate the main findings.

TODO: Provide some code to automate parts of this.

-   Some possible plots

    -   density/histogram plot of X/y distribution
    -   correlation heatmap
    -   pair plots
    -   dimension reduction plots

-   For inspiration: [Shiny App](https://tiffanymtang.shinyapps.io/ShinyEDA/)

# Prediction Modeling

Assuming the goal is prediction...

-   Discuss the prediction methods under consideration, and explain why these methods were chosen.
-   Discuss the accuracy metrics under consideration, and explain why these metrics were chosen.
-   Note: there should be multiple methods and metrics under consideration to paint a more holistic picture of the data.

## Data splitting

-   Split the data into a training, validation, and test set.
-   Decide on the proportion of data in each split.
-   Decide on the "how" to split the data (e.g., random sampling, stratified sampling, etc.), and explain why this is a reasonable way to split the data.

TODO: Provide some code to do the data splitting.

## Prediction check

-   Carry out the prediction pipeline, outlined above.

    -   Fit prediction methods on training data.
    -   Evaluate prediction methods on validation data.
    -   Compare results, and filter out poor models.

TODO: Provide some example code (for both fitting and visualizing results) here so that the practitioner can easily input their data and models.

## Stability check

-   Taking the prediction methods that pass the prediction check, perform stability analysis.

    -   Specify the data perturbation(s).
    -   Re-fit the prediction methods on these perturbed data sets.
    -   Evaluate prediction methods on validation data.
    -   Assess stability across the data perturbations as well as across the various methods.
    -   Interpret stability results.

TODO: Provide some example code here (for both fitting and visualizing results) so that the practitioner can easily input their data and models.

## Interpretability

-   For the models that pass the prediction and stability checks,

    -   Extract the important features in the predictive models that are stable across both data and model perturbations.

TODO: Provide some example code here (for both fitting and visualizing results) so that the practitioner can easily input their data and models.

# Main Results

-   Interpret and summarize the prediction and stability results.
-   Fit pipeline on test data.
-   Summarize test set prediction and/or interpretability results.

# Post hoc analysis

-   Move beyond the global prediction accuracy metrics and dive deeper into individual-level predictions, i.e., provide a more "local" analysis.

    -   Examine any points that had poor predictions.
    -   Examine differences between prediction methods.

# Conclusions

-   Reiterate main findings, and clearly translate findings/analysis back to the domain problem context.

# Bibliography {.unnumbered}
