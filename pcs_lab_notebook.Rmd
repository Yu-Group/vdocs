---
title: "PCS Documentation"
author: ""
date: "`r format(Sys.time(), '%B %d, %Y')`"
header-includes:
    - \usepackage{float}
    - \usepackage{amsmath}
    - \usepackage{gensymb}
    - \usepackage{tcolorbox}
output:
  rmdformats::material:
    fig_caption: true
    code_folding: hide
    number_sections: true
    use_bookdown: true
    fig_width: 10
    fig_height: 8
    lightbox: true
    code_download: true
    includes:
      before_body: html/setup.html
  pdf_document:
    number_sections: true
params:
  ## INPUT PARAMETERS HERE OR USING "Knit with Parameters..." IN ABOVE KNIT MENU
  X_filepath:
    label: "X Data"
    value: data/tcga_brca_array_data.rds
    input: file 
  y_filepath:
    label: "y data"
    value: data/tcga_brca_subtypes.rds
    input: file
  train_prop:
    label: "Training data proportion"
    value: 0.6
    input: numeric
  valid_prop:
    label: "Validation data proportion"
    value: 0.2
    input: numeric
  test_prop:
    label: "Test data proportion"
    value: 0.2
    input: numeric
  modeling_pkg:
    label: "Modeling Package"
    value: "tidymodels"
    input: select
    choices: ["caret", "h2o", "tidymodels"]
  seed:
    label: "Random Seed"
    value: 12345
    input: numeric
css: [css/custom_material_rmd_theme.css, css/custom_checkbox.css]
# runtime: shiny
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
source("scripts/rmd-setup.R", local = knitr::knit_global())

# load in packages
library(magrittr)  # can remove once package is created and `%>%` is imported
for (f in list.files("R", pattern = ".R", full.names = TRUE)) {
  source(f, chdir = T)
}

# set seed
set.seed(params$seed)

# validate inputs
validateDataSplit(params$train_prop, params$valid_prop, params$test_prop)

# load data in
X <- loadFile(params$X_filepath)
y <- loadFile(params$y_filepath)
validateData(X, y)

# initialize counter for subchunkify
chunk_idx <- 1
```

# Domain problem formulation

What is the real-world question? This could be hypothesis-driven or discovery-based.

```{asis, help = TRUE}
Some advice
```

```{asis, interactive_text = TRUE}

```

Why is this question interesting and important? What are the implications of better understanding this data?

```{asis, interactive_text = TRUE}

```

Briefly describe any background information necessary to understand this problem.

```{asis, help = TRUE}
Some advice
```

```{asis, interactive_text = TRUE}

```

Briefly describe how this question can be answered in the context of a model or analysis.

```{asis, interactive_text = TRUE}

```

Outline the rest of the report/analysis.

```{asis, interactive_text = TRUE}

```

# Data

What is the data under investigation? Provide a brief overview/description of the data.

```{asis, interactive_text = TRUE}

```

Describe how your data connects to the domain problem.

```{asis, interactive_text = TRUE}

```

## Data Collection

How was the data collected or generated (including details on the experimental design)? Be as transparent as possible so that conclusions made from this data are not misinterpreted down the road.

```{asis, interactive_text = TRUE}

```

Describe any limitations when using the data to answer the domain problem of interest.

```{asis, interactive_text = TRUE}

```

Where is the data stored, and how can it be accessed by others (if applicable)?

```{asis, interactive_text = TRUE}

```

## Data Splitting

TODO: add advice for possible data splits, AK getting nice figure together

Decide on the proportion of data in each split.

Decide on the "how" to split the data (e.g., random sampling, stratified sampling, etc.), and explain why this is a reasonable way to split the data.

```{asis, interactive_text = TRUE}

```

Split the data into a training, validation, and test set.

```{r split-data}
data_split <- dataSplit(X = X, y = y, stratified_by = y,
                        train_prop = params$train_prop, 
                        valid_prop = params$valid_prop, 
                        test_prop = params$test_prop)
Xtrain <- data_split$X$train
Xvalid <- data_split$X$validate
Xtest <- data_split$X$test
ytrain <- data_split$y$train
yvalid <- data_split$y$validate
ytest <- data_split$y$test
```

Provide summary statistics and/or figures of the three data sets to illustrate how similar (or different) they are.

```{asis, interactive_text = TRUE}

```

### Data Splitting Overview {.tabset .custom-tabs}

#### X Data Split {.unnumbered}

```{r X-data-split, add_new_line = TRUE}
plotDataSplit(Xtrain, Xvalid, Xtest, 
              xlab = "X", title = "Overall X Distribution")
```

#### Y Data Split {.unnumbered}

```{r y-data-split, add_new_line = TRUE}
plotDataSplit(ytrain, yvalid, ytest, 
              xlab = "y", title = "Overall y Distribution")
```

## Data Cleaning and Preprocessing

What steps were taken to clean the data? More importantly, why was the data cleaned in this way?

Discuss all inconsistencies, problems, oddities in the data (e.g., missing data, errors in data, outliers, etc.).

Record your preprocessing steps in a way such that if someone else were to reproduce your analysis, they could easily replicate and understand your steps.

It can be helpful to include relevant plots that explain/justify the choices that were made when cleaning the data.

If more than one preprocessing pipeline is reasonable, examine the impacts of these alternative preprocessing pipelines on the final data results.

Again, be as transparent as possible. This allows others to make their own educated decisions on how best to preprocess the data.

```{asis, interactive_text = TRUE}
Given this example TCGA BRCA data set, we first preprocess the data by removing constant or duplicated columns. Then since the array data is highly right skewed, we will log-transform (i.e., log(x + 1)) the data. Finally, to keep this example template relatively quick to run, we will only keep the 1000 features with the highest variance.
```

```{r preprocess-data}
## DO DATA CLEANING / PRE-PROCESSING HERE 
Xtrain <- log(Xtrain + 1) %>%
  removeConstantCols(verbose = 1) %>%
  removeDuplicateCols(verbose = 1) %>%
  filterColsByVar(max_p = 1000)
Xvalid <- log(Xvalid + 1)[, colnames(Xtrain)]
Xtest <- log(Xtest + 1)[, colnames(Xtrain)]
```


## Data Exploration

TODO: Add drag and drop feature in shiny version for other images

The main goal of this section is to give the reader a feel for what the data "looks like" at a basic level.

Provide plots that summarize the data and perhaps even plots that convey some smaller findings which ultimately motivate the main findings.

Provide additional plots representing remaining oddities after pre-processing if applicable.

Add summary statistics in accompanying tables (or in figures) for quick comparisons.

```{asis, interactive_text = TRUE}

```

### Data Overview {.tabset .custom-tabs}

```{r data-dims, echo = FALSE}
# summary of data dimensions
dataDimensions(Xtrain = Xtrain, Xvalid = Xvalid, Xtest = Xtest)
```

```{r data-types, results="asis", add_new_line = TRUE}
# summary of types of features in (X, y) data
dataTypes(X = Xtrain, y = ytrain)
```

#### Summary Tables {.unnumbered}

```{r data-summary-table, results="asis", add_new_line = TRUE}
# broad array of summary statistics for features in training (X, y)
tab_ls <- dataSummary(X = Xtrain, y = ytrain)
for (dtype in names(tab_ls)) {  # grouped by data type
  simChef:::subchunkify(tab_ls[[dtype]], i = chunk_idx, 
                        other_args = "results='asis'")
  chunk_idx <- chunk_idx + 1
}
```

#### X Distribution {.unnumbered}

```{r x-dist-plot, add_new_line = TRUE}
# plot X distribution
plotDataDistribution(data = Xtrain, 
                     xlab = "X", title = "Training X Distribution")
```

#### Y Distribution {.unnumbered}

```{r y-dist-plot, add_new_line = TRUE}
# plot y distribution
plotDataDistribution(data = ytrain, 
                     xlab = "y", title = "Training y Distribution")
```

#### Data Heatmap {.unnumbered}

```{r data-heatmap, add_new_line = TRUE}
# (clustered) heatmap of (X, y) data
plotDataHeatmap(X = Xtrain, y = ytrain, clust_rows = TRUE, clust_cols = TRUE,
                show_ytext = FALSE, x_text_angle = TRUE)
```

#### Feature Correlation {.unnumbered}

```{r feature-cor-heatmap, add_new_line = TRUE}
# (clustered) correlation heatmap
plotCorHeatmap(X = Xtrain, cor_type = "pearson", clust = TRUE,
               x_text_angle = TRUE) +
  ggplot2::labs(x = "Features", y = "Features", fill = "Cor.")
```

#### Feature Pair Plots {.unnumbered}

```{r feature-pair-plot, add_new_line = TRUE}
# feature pair plot (for a subset of features)
keep_features <- sort(
  sample(1:ncol(Xtrain), min(ncol(Xtrain), 6), replace = FALSE)
)
plotPairs(data = Xtrain, columns = keep_features, 
          color = ytrain, color_label = "y")
```

#### Marginal Associations {.unnumbered}

```{r marginal-assoc-plot, add_new_line = TRUE}
# marginal association plots (for a subset of features)
caret::featurePlot(x = Xtrain[, keep_features],
                   y = ytrain,
                   plot = if (is.factor(ytrain)) "box" else "scatter",
                   scales = list(x = list(relation = "free"), 
                                 y = list(relation = "free")))
```

#### PCA {.unnumbered}

```{r pca-plot, add_new_line = TRUE}
# pca plot
plotPCA(X = Xtrain, npcs = 3, color = ytrain, color_label = "y",
        center = TRUE, scale = FALSE)$plot
```

# Prediction Modeling

TODO: add advice on which models to select and why

Discuss the prediction methods under consideration, and explain why these methods were chosen.

```{asis, interactive_text = TRUE}

```

Discuss the accuracy metrics under consideration, and explain why these metrics were chosen.

```{asis, interactive_text = TRUE}

```

Note: there should be multiple methods and metrics under consideration to paint a more holistic picture of the data. At least one method should be a baseline, common approach that may not be optimal for the problem setting, but serves as a helpful comparison.

## Prediction check {.tabset .custom-tabs}

Carry out the prediction pipeline, outlined above.

1. Fit prediction methods on training data.
2. Evaluate prediction methods on validation data.
3. Compare results, and filter out poor models.

```{asis, interactive_text = TRUE}

```

```{r caret-fit-params, eval = params$modeling_pkg == "caret", echo = params$modeling_pkg == "caret", cache = FALSE}
## IF USING CARET MODELING BACKEND, CHOOSE METHODS AND TRAINING CONTROLS HERE
cv_options <- list(nfolds = 5, 
                   foldids = NULL,
                   metric = "Accuracy")

model_list <- list(
  ranger = list(.tune_params = list(mtry = round(seq(sqrt(ncol(Xtrain)), 
                                                     ncol(Xtrain) / 3,
                                                     length.out = 3)),
                                    splitrule = "gini",
                                    min.node.size = 1),
                importance = "impurity",
                num.threads = 1),
  xgbTree = list(nthread = 1)
)
```

```{r h2o-fit-params, eval = params$modeling_pkg == "h2o", echo = params$modeling_pkg == "h2o", cache = FALSE}
## IF USING H2O MODELING BACKEND, CHOOSE METHODS AND TRAINING CONTROLS HERE
require(h2o)
cv_options <- list(nfolds = 5,
                   foldids = NULL,
                   metric = "accuracy")

model_list <- list(
  randomForest = list(.tune_params = list(mtries = round(seq(sqrt(ncol(Xtrain)),
                                                             ncol(Xtrain) / 3,
                                                             length.out = 3))),
                      ntrees = 500),
  xgboost = list()
)

# initialize h2o cluster
h2o.init(nthreads = -1)
```

```{r tidymodels-fit-params, eval = params$modeling_pkg == "tidymodels", echo = params$modeling_pkg == "tidymodels", cache = FALSE}
## IF USING TIDYMODELS BACKEND, CHOOSE METHODS AND TRAINING CONTROLS HERE
cv_options <- list(nfolds = 5,
                   foldids = NULL,
                   metric = "accuracy")

model_list <- list(
  rand_forest = list(.tune_params = list(mtry = round(seq(sqrt(ncol(Xtrain)),
                                                          ncol(Xtrain) / 3,
                                                          length.out = 3))),
                     engine = list(engine = "ranger",
                                   importance = "impurity")),
  boost_tree = list(engine = "xgboost")
)
```

```{r fit-models, class.output="scroll-300"}
# fit/train models
fit_results <- fitModels(Xtrain = Xtrain, ytrain = ytrain,
                         model_list = model_list, cv_options = cv_options,
                         use = params$modeling_pkg)

# make prediction on validation set
pred_results <- predictModels(fit_list = fit_results, Xtest = Xvalid)

# evaluate predictions on validation set
eval_results <- evaluateModels(pred_df = pred_results, ytest = yvalid)

# collect feature importance metrics from model fits
imp_results <- interpretModels(fit_list = fit_results)
```


### Fit Summary {.unnumbered .tabset .tabset-pills}

```{r fit-summary, echo = FALSE, results = "asis"}
# print out fit output summary
printFitResults(fit_results)
```

### Prediction Results {.unnumbered}

```{r prediction-results, echo = FALSE, results = "asis"}
# display prediction metrics, confusion matrices, and roc/pr plots if applicable
showEvalResults(eval_results, digits = 2, sigfig = FALSE,
                html_options = list(options = list(dom = "t")))
```

## Stability check {.tabset .custom-tabs}

Taking the prediction methods that pass the prediction check, perform stability analysis.

1. Specify and justify the appropriate data perturbation(s).
2. Re-fit the prediction methods on these perturbed data sets.
3. Evaluate prediction methods on validation data.
4. Assess stability across the data perturbations as well as across the various methods.
5. Filter out poor models where necessary and interpret stability results.

```{asis, interactive_text = TRUE}

```

<!-- TODO: Ana - Provide some example code here (for both fitting and visualizing results) so that the practitioner can easily input their data and models. Add something like James' slide. A few pictures with possible data perturbation schemes (separate from parameter tuning). Data splitting vs sampling of observations. Cross-validation-ish scheme and fixed training/validation setup (bootstrapping, sub-sampling, stratified-sampling). Add parameter to include/exclude certain code chunks. -->

```{r stability-check}
n_reps <- 2 # increase for better stability measures when not testing code

# p <- progressr::progressor(steps = n_reps)
# future::plan(multisession, workers = min(n_reps, parallel::detectCores() - 1))
bootstrap_model_results <- future.apply::future_replicate(
  n = n_reps,
  expr = {
    bootstrap <- sample(1:nrow(Xtrain), nrow(Xtrain), replace = TRUE)
    Xtrain_b <- Xtrain[bootstrap, ]
    ytrain_b <- ytrain[bootstrap]
    
    # fit/train models on bootstrap data
    fit_results_b <- fitModels(Xtrain = Xtrain_b, ytrain = ytrain_b,
                               model_list = model_list, cv_options = cv_options,
                               use = params$modeling_pkg)

    # make prediction on validation set
    pred_results_b <- predictModels(fit_list = fit_results_b, Xtest = Xvalid)

    # evaluate predictions on validation set
    eval_results_b <- evaluateModels(pred_df = pred_results_b, ytest = yvalid)
    
    # collect feature importance metrics from model fits
    imp_results_b <- interpretModels(fit_list = fit_results_b)
    
    return(list(# fit = fit_results_b,  # uncomment if need bootstrap fits; can be memory intensive
                predictions = pred_results_b,
                eval_metrics = eval_results_b,
                importances = imp_results_b))
  },
  simplify = FALSE
)

bootstrap_model_errs <- purrr::map_dfr(bootstrap_model_results,
                                       ~.x$eval_metrics$metrics,
                                       .id = "bootstrap_id")
bootstrap_model_preds <- purrr::map(bootstrap_model_results, "predictions")
bootstrap_model_imps <- purrr::map_dfr(bootstrap_model_results, "importances",
                                       .id = "bootstrap_id")

# summarize bootstrap model error metrics
bootstrap_model_errs_summary <- bootstrap_model_errs %>%
  tidyr::pivot_longer(cols = c(-bootstrap_id, -Metric), 
                      names_to = "Method", values_to = "Value") %>%
  dplyr::group_by(Method, Metric) %>%
  dplyr::summarise(Mean = mean(Value), SD = sd(Value), .groups = "drop") %>%
  tidyr::pivot_longer(cols = c(Mean, SD), 
                      values_to = "Value", names_to = "Statistic") %>%
  tidyr::pivot_wider(names_from = "Method", values_from = "Value") %>%
  dplyr::arrange(Metric, Statistic)
```

### Table {.unnumbered}

```{r stability-check-table, results = "asis"}
# table of accuracy metrics
prettyTable(
  bootstrap_model_errs_summary, 
  digits = 2, sigfig = FALSE, rownames = FALSE,
  caption = "Validation Prediction Accuracies Over Bootstrapped Training Fits", 
  html_options = list(
    extensions = "RowGroup",
    options = list(
      dom = "t",
      rowGroup = list(dataSrc = 0),
      columnDefs = list(list(className = "dt-center", targets = "_all"),
                        list(visible = FALSE, targets = 0),
                        list(title = "Metric", targets = 1))
    )
  )
)
```

### Plot {.unnumbered}

```{r stability-check-boxplot}
# boxplots
bootstrap_model_errs %>%
  tidyr::pivot_longer(cols = c(-bootstrap_id, -Metric), 
                      names_to = "Method", values_to = "Value") %>%
  plotBoxplot(x_str = "Method", y_str = "Value") +
  ggplot2::facet_wrap(~ Metric) +
  ggplot2::labs(title = "Validation Prediction Accuracies Over Bootstrapped Training Fits")
```

## Interpretability {.tabset .custom-tabs}

For the models that pass the prediction and stability checks, extract the important features in the predictive models that are stable across both data and model perturbations. Determining the importance of a feature can be method dependent.

```{asis, interactive_text = TRUE}

```

### Full Model (without stability) {.unnumbered .tabset .tabset-pills}

#### Table {.unnumbered}

```{r interpretability-table, add_new_line = TRUE, results = "asis"}
prettyTable(imp_results, digits = 2, sigfig = FALSE,
            caption = "Variable Importances")
```

#### Plots {.unnumbered}

```{r interpretability-plot, add_new_line = TRUE}
plotFeatureImportance(imp_results,
                      use_rankings = FALSE,
                      use_facets = TRUE,
                      interactive = FALSE)
```

```{r interpretability-pair-plot, add_new_line = TRUE}
plotFeatureImportancePair(imp_results,
                          use_rankings = TRUE,
                          interactive = FALSE)
```

### Bootstrapped Model (with stability) {.unnumbered .tabset .tabset-pills}

#### Table {.unnumbered}

```{r boot-interpretability-table, add_new_line = TRUE, results = "asis"}
bootstrap_model_imps_summary <- bootstrap_model_imps %>%
  dplyr::group_by(Method, Variable) %>%
  dplyr::summarise(`Mean Importance` = mean(Importance),
                   `Median Importance` = median(Importance),
                   `SD Importance` = sd(Importance),
                   `Min Importance` = min(Importance),
                   `Max Importance` = max(Importance), 
                   .groups = "keep")
prettyTable(
  bootstrap_model_imps_summary, 
  digits = 2, sigfig = F, 
  caption = "Summary of variable importances across bootstrapped models"
)
```

#### Plots {.unnumbered}

```{r boot-interpretability-plot, add_new_line = TRUE}
plotFeatureImportanceStability(bootstrap_model_imps,
                               use_rankings = FALSE,
                               use_facets = TRUE,
                               interactive = FALSE)
```

# Main Results

## {.unnumbered .tabset .custom-tabs}

Interpret and summarize the prediction and stability results.

```{asis, interactive_text = TRUE}

```

Evaluate pipeline on test data.

```{r final-fits, class.output="scroll-300"}
Xtrain_final <- dplyr::bind_rows(Xtrain, Xvalid)
ytrain_final <- c(ytrain, yvalid)

# fit/train models
fit_results_final <- fitModels(Xtrain = Xtrain_final, ytrain = ytrain_final,
                               model_list = model_list, cv_options = cv_options,
                               use = params$modeling_pkg)

# make prediction on test set
pred_results_final <- predictModels(fit_list = fit_results_final, Xtest = Xtest)

# evaluate predictions on test set
eval_results_final <- evaluateModels(pred_df = pred_results_final, ytest = ytest)

# collect feature importance metrics from model fits
imp_results_final <- interpretModels(fit_list = fit_results_final)
```

Summarize test set prediction and/or interpretability results.

```{asis, interactive_text = TRUE}

```

<!-- TODO: Ana - add template tables with interpretation -->

### Fit Summary {.unnumbered .tabset .tabset-pills}

```{r final-fit-summary, echo = FALSE, results = "asis"}
# print out fit output summary
printFitResults(fit_results_final)
```

### Prediction Results {.unnumbered}

```{r final-prediction-results, echo = FALSE, results = "asis"}
# display prediction metrics, confusion matrices, and roc/pr plots if applicable
showEvalResults(eval_results_final, test_set = TRUE, digits = 2, sigfig = FALSE,
                html_options = list(options = list(dom = "t")))
```

# Post hoc analysis

Move beyond the global prediction accuracy metrics and dive deeper into individual-level predictions for the validation and/or test set, i.e., provide a more "local" analysis.

-   Examine any points that had poor predictions.
-   Examine differences between prediction methods.

```{asis, interactive_text = TRUE}

```

```{r posthoc-pair-plot}
pred_results_final %>%
  dplyr::mutate(.id = rep(1:nrow(Xtest), length.out = dplyr::n())) %>%
  tidyr::pivot_wider(id_cols = .id, 
                     names_from = "Method", values_from = "predictions") %>%
  dplyr::mutate(`True Responses` = ytest) %>%
  plotPairs(columns = 2:(length(unique(pred_results_final$Method)) + 2),
            title = "Comparison of model test predictions")
```

# Conclusions

Reiterate main findings, note any caveats, and clearly translate findings/analysis back to the domain problem context.

```{asis, interactive_text = TRUE}

```

