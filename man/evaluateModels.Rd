% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modeling.R
\name{evaluateModels}
\alias{evaluateModels}
\title{Wrapper for evaluating predictions from fitted models using common modeling
backends}
\usage{
evaluateModels(pred_df, ytest, metrics = NULL, na_rm = TRUE)
}
\arguments{
\item{pred_df}{Data frame of predictions to evaluate against `ytest`.
Typically the output of `predictModels()`.}

\item{ytest}{Test response vector for which to evaluate against the
predictions.}

\item{metrics}{A `metric_set` object indicating the metrics to evaluate. See
`yardstick::metric_set()` for more details. Default `NULL` will use a
default set of metrics that depends on the type of problem (e.g.,
classification vs regression).}

\item{na_rm}{Logical indicating whether `NA` values should be stripped before
the computation proceeds.}
}
\value{
A list with the following elements:
\describe{
\item{metrics}{Name of method that made the associated prediction.}
\item{conf}{In a classification problem, this is a tibble with the confusion
  matrices for each method (see output of `yardstick::conf_mat()`). This
  element is omitted for regression problems.}
\item{roc_plot}{In a classification problem with the predicted probabilities
  provided in `pred_df`, this is a ggplot object with the ROC evaluation
  plot. This element is omitted for regression problems or if the predicted
  probabilities are not provided.}
\item{pr_plot}{In a classification problem with the predicted probabilities
  provided in `pred_df`, this is a ggplot object with the PR evaluation
  plot. This elemented is omitted for regression problems or if the predicted
  probabilities are not provided.}
}
}
\description{
`evaluateModels` is a wrapper function for evaluating
  predictions from multiple fitted models that were trained using caret,
  tidymodels, or h2o backends. This wrapper function provides uniformity of
  input arguments to easily switch between the different modeling packages.
}
